\chapter{Lessons Learned and Future Work}
\label{chapter-lessons}

This chapter summarizes some key observations and lessons that have emerged
from the work described in this dissertation. We describe a set of important
takeaways from our experiences deploying real systems, while also identifying
some strengths and weaknesses of two architectures described in the preceding
chapters. We also briefly outline areas for future work.

\section{Lessons Learned}

Broadly speaking, we can divide the takeaways mentioned here into two
categories. The first set relate to why and how to build applications and
perform field deployments. These have emerged from the process of building
and deploying three solutions in the context of the volcano monitoring
application. The second set comment on the strengths and weaknesses of the
two architectural solutions to sensor network resource management we have
presented: Lance and IDEA.

\subsection{Deployment Lessons}

Sensor network deployments, particularly in remote areas, involve significant
cost in terms of time and equipment. Failures of hardware and software can
have a negative impact on the uptake of this technology by domain science
experts. Our experiences at Ecuadorean volcanos have yielded a number of
valuable lessons for future sensor network deployments. 

\begin{itemize}

\item \textbf{Ground truth and self-validation mechanisms are critical.}
During our 2005 deployment, we did not initially consider colocating several
of our wireless sensors with existing data loggers in order to establish
ground truth. This would have clearly aided our analysis, though we were
fortunate to locate one of our sensors near (but not immediately adjacent to)
the RVEN station. In addition, self-validation mechanisms are needed to
provide detailed information on the health and accuracy of the data recorded
by the network. The periodic ``heartbeat'' messages that we built into our
system proved essential to remotely tracking system operation.

More generally, it is critical to design the evaluation process well before
the system being studied is designed and deployed. Deployments are expensive
and deployment time is valuable, and if the system is not properly
instrumented it can be difficult to assess its performance after the
deployment has ended.

\item \textbf{Coping with infrastructure and protocol failures.} As discussed
previously, in 2005 we were surprised to find that the sensor nodes
themselves were the most reliable components of the system. Even without
classifying the 3-day network outage as an infrastructure failure, this
downtime was far exceeded by outages caused by power failures at the base
station. We did not devote enough attention to assuring the reliability of
the base station and radio modem infrastructure, assuming it would be a
trivial matter of plugging into wall power. This single point of failure was
more fragile than expected.

Additionally, several pieces deployed software, including Deluge and FTSP,
exhibited failures in the field than we not had expected given our laboratory
experiments. These failures both speak for and show the limitations of
careful, pre-deployment testing. While we were fortunate to be able to
correct protocol errors in the field and during post-processing, the risk
of uncorrectable problems argues for more rigorous testing and analysis. At
the same time, the unpredictability of field deployments places demands on
the flexibility and adaptability of the researchers involved.

\item \textbf{Building confidence inside cross-domain scientific
collaborations.} It is important when working with domain scientists to
understand their expectations and plan carefully to meet them. There can be
tensions between the desire of computer science researchers to develop more
interesting, sophisticated and complex systems, and the needs of domain
science, which relies upon thoroughly validated instrumentation. Pushing more
complexity into the sensor network can improve lifetime and performance, but
the resulting system must be carefully validated before deployment to ensure
that the resulting data is scientifically accurate.

Good communication between computer and domain scientists is also critical.
During the 2005 deployment, the seismologists were eager to see the collected
signals, which were initially in an unprocessed format with timing errors as
described earlier. From the CS perspective, the early data provided evidence
of successful data collection, but from the geophysics perspective it
highlighted failures in the time synchronization protocol. It took a great
deal of effort after the deployment to build confidence in the validity of
our data.

Finally, the development of Lance required a great deal of exchange between
te domain and computer scientists involved. Seismologists are used to systems
able to provide complete, high-fidelity signals from all stations spanning
all time intervals. The capabilities of the instruments that they typically
deploy have meant that they have not had to think about how to classify data
once not all signals have been collected. Although they were initially quite
hesitant, we were able to convince them that the easy of deployment and
promise of increased spatial resolution provided by our low-power devices
made it worth abandoning complete temporal coverage. Once convinced, they
were very helpful in suggesting ways to prioritize collected signals and
suggested the node-level summarization functions used during the 2007
deployment.

\end{itemize}

\subsection{Architectural Lessons}

\begin{itemize}

\item \textbf{Balance centralization and decentralization.}

\item \textbf{Energy is the most important resource.}

\item \textbf{Treat the entire network as a single instrument.}

\item \textbf{Connect node-level behavior with application-level fidelity.}

\end{itemize}

\section{Future Work}

Despite advances in this area, sensor networks have just begun to make
inroads in augmenting and replacing existing scientific instrumentation. The
macroscope is still a new instrument, and requires much more research and
development before it can truly deliver the kind of views envisioned by de
Rosnay.

Opportunities remain for collaborations between sensor network researchers
and domain scientists in many areas. As described in
Chapter~\ref{chapter-related}, parallel efforts have already succeeded in
leveraging this technology to study animal habitats and behavior and a
variety of natural environments. Moving forward we expect to see these kinds
of scientific sensor networks defining an unique research agenda, with
emphasis on scaling out to thousands or millions of nodes, delivering large
amounts of high-fidelity data, and achieving perpetual operation predicated
on improved energy-harvesting capabilities.

These goals will continue to place pressure on key system capabilities
developed by the research described in this dissertation. Resources will
still be precious, and must be devoted to the most important information; and
energy availability will vary across the network in unpredictable ways,
potentially threatening high-fidelity operation.

The Lance architecture described in Chapter~\ref{chapter-lance} was broadened
and decentralized to produce IDEA, but several core challenges still remain.
Lance's linear policy modules are easy to use and compose, but it remains
unclear whether more complex interactions between policy modules are needed.
During the process of adapting Lance to support medical monitoring the policy
module architecture was revisted and extended, but incorporating additional
applications may require additional changes and more generality.
Additionally, Lance's reliance on a central controller limits the scalibility
of the architecture. While IDEA allows certain kinds of resource-management
decisions to be made in a distributed fashion, it does not completely
eliminate the need for complete network visibility when trying to optimize
certain tasks. Instead of aiming at a completely flat network, Lance may
adapt well to tiered systems that achieve scalability without moving nodes
too far away from devices with significant computational capabilities.

As far as IDEA, for future work we are interested in addressing the problem
of cross-component interaction which would allow us to optimize the operation
of several IDEA components running in the network simultaneously. This is
complicated by the fact that there is likely to be dependencies between
components that cause decisions made by one to affect others. As an example,
the LPL intervals used by a node would effect the power cost to use the link
seen by the routing protocol. Considering cross-component interaction also
rapidly expands the search space when nodes try to identify the right local
state to select. Currently we assume that the search space is small enough
that we can search it exhaustively. Cross-component support may require that
we develop more intelligent search strategies in order to allow this process
to be performed on computationally-constrained devices.

In addition we are investigating ways to model the impact of node failure on
other nodes. Many sensor network protocols will try to work around nodes
leaving the network or going offline, but this repair process is costly and
causes load within the network to shift in ways that are difficult to
anticipate \textit{a priori}. One option here is to use network-level
simulators running in parallel with the deployed system itself. Information
about the deployment environment can be harvested continuously to increase
the reality of the simulated outcomes. When trying to adjust network
behavior, the impact of various decisions could be evaluated quickly in
simulation incorporating the effect of node failures.
