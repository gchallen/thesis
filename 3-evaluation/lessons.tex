\section{Deployment Lessons Learned}
\label{evaluation-sec-lessons}

Sensor network deployments, particularly in remote areas, involve significant
cost in terms of time and equipment. Failures of hardware and software can
have a negative impact on the uptake of this technology by domain science
experts. Our experiences at Reventador volcano produced a set of valuable
lessons for future sensor network deployments. Several important lessons led
directly to Lance (Chapter~\ref{chapter-lance}) and IDEA
(Chapter~\ref{chapter-idea}), new architectural approaches described in later
chapters.

\begin{itemize}

\item \textbf{Working with developmental code:} The software failures that we
observed during our deployment (FTSP, Deluge) are common to code contributed
by research projects. Frequently it takes a great deal of time between when
the code is made publicly available and all of the errors and problems are
removed.

\hspace{0.25in} In the case of FTSP, we tried to do the right thing by
careful experimentation before we deployed our system, but were still
surprised by its poor performance in the field. In the case of Deluge, we
were guilty of incorporating it into the system extremely late, and did not
have time to test it as rigorously as other components. In addition, our lack
of experience with the Deluge code meant that, when it failed, we had no
choice but to remove it and try to reproduce some of the important
functionality it was providing elsewhere.

\item \textbf{Ground truth and self-validation mechanisms:}
During our deployment, we did not initially consider colocating several
of our wireless sensors with existing data loggers in order to establish
ground truth. This would have clearly aided our analysis, though we were
fortunate to locate one of our sensors near (but not immediately adjacent to)
the RVEN station. In addition, self-validation mechanisms are needed to
provide detailed information on the health and accuracy of the data recorded
by the network. The periodic ``heartbeat'' messages that we built into our
system proved essential to remotely tracking system operation.

\hspace{0.25in} More generally, it is critical to design the evaluation
process well before the system being studied is designed and deployed.
Deployments are expensive and deployment time is valuable, and if the system
is not properly instrumented it can be difficult to assess its performance
after the deployment has ended.

\item \textbf{Coping with infrastructure and protocol failures:} As discussed
previously, we were surprised to find that the sensor nodes themselves were
the most reliable components of the system. Even without classifying the
3-day network outage as an infrastructure failure, this downtime was far
exceeded by outages caused by power failures at the base station. We did not
devote enough attention to assuring the reliability of the base station and
radio modem infrastructure, assuming it would be a trivial matter of plugging
into wall power. This single point of failure was more fragile than expected.

\hspace{0.25in} Additionally, several pieces of deployed software, including
Deluge and FTSP, exhibited failures in the field that we had not expected
given our laboratory experiments. These failures both speak for and show the
limitations of careful, pre-deployment testing. While we were fortunate to be
able to correct protocol errors in the field and during post-processing, the
risk of uncorrectable problems argues for more rigorous testing and analysis.
At the same time, the unpredictability of field deployments places demands on
the flexibility and adaptability of the researchers involved.

\item \textbf{Building confidence inside cross-domain scientific
collaborations:} It is important when working with domain scientists to
understand their expectations and plan carefully to meet them. There can be
tensions between the desire of computer science researchers to develop more
interesting, sophisticated and complex systems, and the needs of domain
science, which relies upon thoroughly validated instrumentation. Developing
new instrumentation means a nontrivial probability of failure, which can be
frustrating to scientists used to working with reliable equipment. Pushing
more complexity into the sensor network can improve lifetime and performance,
but the resulting system must be carefully validated before deployment to
ensure that the resulting data is scientifically accurate.

\hspace{0.25in} Effective collaboration involves both meeting and managing
scientific expectations. Particularly when using low-power devices, it can be
difficult to duplicate the properties of existing high-power instrumentation.
In our case, we were unable to provide continuous signals for analysis.
Understanding the scientists' goals makes it more likely that you can
convince them that your system can meet them, despite the fact that it is
probably providing different information with different properties than they
are used to. Simply duplicating existing instrumentation is not a path to the
macroscope. Scientists also have to devise new techniques for harnessing
low-power, unreliable, embedded devices fitted with low-resolution sensors.

\hspace{0.25in} Good communication between computer and domain scientists is
also critical. During our deployment, the seismologists were eager to see the
collected signals, and we were eager to have scientific feedback about
whether or not the data we were collecting was useful. Volcano scientist have
a suite of commonly-used visualization and analysis tools. However, we were
not ready to convert our data into a format that they could use. In fact,
this only happened several months after we returned. Instead, we quickly
built several tools ourselves, but their limitations and the fact that the
volcanologists didn't use them hampered their ability to help us analyze our
data.

\hspace{0.25in} In our eagerness to present some data to our collaborators,
we also initially showed them unprocessed data containing the timing errors
mentioned earlier. From our perspective, this early data provided evidence
that many components of our system --- such as sampling, routing, and data
collection --- were operating properly. However, from the geophysics
perspective it highlighted failures in the time synchronization protocol. It
took a great deal of effort after the deployment to rebuild confidence in the
validity of our data that we had damaged through these early viewings.

\item \textbf{Incorporating flexibility for experimentation and system
adaptation:} Designing a system that is flexible and provides a great deal of
diagnostic output can be a great way to facilitate deployment
experimentation. It also has the side benefit of making it more likely that
researchers can work around unforseen failures.

\hspace{0.25in} Our system included two features that improved flexibility
and visibility and that we found very helpful. First was the
command-and-control interface discussed earlier. We used the ability to
reboot nodes remotely to help work around the FTSP failures, and the ability
to change node state could have helped if other protocols had not worked as
expected. For example, had the routing protocol failed or produced poor
performance we could have chosen each node's parent manually. Second, the
periodic status messages provided critical information about the functioning
of the system. Despite the rapid rate at which they were being sent (every
10~s), their overhead did not have an appreciable effect on the system's
performance, and the ability to quickly view the effects of changes to the
network was extremely helpful when debugging.

\hspace{0.25in} Given more time it would have been interesting to experiment
with other system components. We could have varied the metrics used by the
routing tree, compared various event detection algorithms, or tweaked
parameters used by the bulk data-transfer protocol.

\item \textbf{Designing the evaluation along with the system:} Many of the
challenges that we faced in evaluating our system and working with our
collaborators in the field could have been averted had we designed the
evaluation along with the system. As it happened, we were racing to complete
the implementation of the system as the deployment deadline neared. Several
important components were not completely finished until we arrived in Ecuador
at the deployment site.

\hspace{0.25in} While our system implementation was itself behind schedule,
none of our timelines had included time needed to design an evaluation
strategy for our field deployment. We did build a great deal of logging
infrastructure into our toolchain in order to collect data which eventually
did prove important for the validation of our system. However, taking the
evaluation design process seriously before deployment would have had several
positive outcomes. First, we would almost certainly have noticed the
ground-truthing problems we were forced to work around later. Second, we
would have developed a set of tools for converting our data into a format
that the seismologists could use, which would improved our interaction with
them at the deployment site and allowed them to help us tune the behavior of
our system. Finally, had we been more confident about our ability to evaluate
the system's performance, we may have been more willing to experiment with
different protocols or parameter settings while on-site.

\hspace{0.25in} For our system and other systems like it, field deployments
are critical and often expensive, and so deployment time is extremely
valuable. Planning how to use it should not be overlooked.

\end{itemize}

Many features of Lance, described in the next chapter, emerged directly out
of some of the challenges we encountered at Reventador. Lance replaces the
binary event detector with a value-based ordering of all signals. This
reduces the dependence of the performance of the overall system on event
detection parameters and thresholds, and allows us to adapt to changing
volcanic behavior. Data collection, which was shown to be a significant
bottleneck in terms of bandwidth and energy consumption, is also considered
directly when examining signals to download. Finally, our negative experience
with Deluge caused us to avoid relying on adaptations that required
reprogramming nodes, leading to the policy-module framework included in Lance
which facilitates significant changes to network behavior through changes
made only at the base station.
