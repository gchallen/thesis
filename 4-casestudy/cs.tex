\section{Sensor Network Application Design}
\label{sec-cs}

Given the current capabilities of wireless sensor network nodes, we set out to
design a data collection network meeting the scientific requirements outlined
above.  Before motivating our design we provide a high-level overview of
network operation.

\subsection{Typical Network Operation}

Each node samples two or four channels of seismoacoustic data at 100~Hz,
storing the data in local flash memory. Nodes also transmit periodic status
messages and perform time synchronization, as described below.
When a node detects an interesting event, it routes a message to the 
base station laptop.  
If enough nodes report an event within a short time interval, the laptop
initiates data collection which proceeds in a round-robin fashion. Between
30~and~60~seconds of data is downloaded from each node with a reliable data
collection protocol, ensuring that all buffered data from the event is
retrieved.  When data collection completes, nodes return to sampling 
and storing sensor data.

\subsection{Overcoming High Data Rates: Event Detection and Buffering}

% GWA : Changed numbers to KBS and fixed 50 Kbits/second number which is
% lowballing it.

When designing high data-rate sensing applications the most important
limitation of current sensor network nodes is low radio bandwidth.  IEEE
802.15.4 radios, such as the Chipcon CC2420, have raw data rates of around
30~KBytes/sec.  However overheads caused by packet framing, medium access
control (MAC), and multihop routing reduce the achievable data rate to less
than 10~KBytes/sec even in a single-hop network.

As a result, nodes can acquire data faster than they can transmit.  Simply
logging data to local storage for later retrieval is also infeasible for
these applications.  The TMote Sky has 1~MByte of flash memory which fills in
around 20 minutes on a two-component sensor node.  Fortunately, many
interesting volcanic events will fit in this buffer.  For a typical
earthquake or explosion at Reventador, 60~sec of data from each node is
adequate.  Therefore, we focused on developing a network to reliably collect
data for short, discrete events.  

% MDW: I don't think we need to use the term "local" event detector
% here. Also, you only need to italicize a term the first time it is
% used.
Sampled data is stored in the local flash memory of the node, which is
treated as a circular buffer. Each block of data is timestamped using
the local node time, which can later be mapped onto a global time,
as explained below. Each node runs an {\em event detector} on 
locally-sampled data. 
Good event detection algorithms produce high
event-detection rates while maintaining small false-positive rates.
The sensitivity of the detection algorithm links these two metrics: a more
sensitive detector correctly identifies more events at the
expense of producing more false positives.  

The data set produced by our previous deployment at Volc\'{a}n
Tungurahua~\cite{volcano-ewsn05} aided in the design of the event detector.
We implemented a short-term average/long-term average threshold detector,
which computes two exponentially-weighted moving averages (EWMAs) with
different gain constants. When the ratio between short-term average and the
long-term average exceeds a fixed threshold, the detector fires.  The
detector threshold allows nodes to distinguish between low-amplitude signals,
perhaps caused by distant earthquakes, and high-amplitude signals caused by
nearby volcanic activity.

When the event detector on a node fires it routes a small message to the base
station laptop.  If enough nodes report events within a certain time window
the laptop initiates data collection from the entire network (including nodes
that did not report the event). This global filtering prevents spurious event
detections from triggering a data collection cycle. Fetching 60~sec of data
from all 16 nodes in the network takes approximately one hour.  Because nodes
can only buffer 20 minutes of eruption data locally each node pauses sampling
and reporting events until its data has been uploaded.  As the latency
associated with data collection prevents our network from capturing all
events, optimizing the data collection process is a focus of future work.

%Buffering on each node is required because the identification and data
%retrieval takes time after which data from an event must remain available on
%the node.  We developed a component allowing us to treat the Flash memory as
%circular buffer.  Because our nodes filled this store in roughly 20 minutes
%and it typically took around one hour to download a single event from the
%entire network we disabled sampling on each node before beginning the global
%retrieval process.  

% GWA : Found a lot of this confusing and bordering on incorrect.  Cleaned up
% a bit.

\subsection{Reliable Data Transmission and Time Synchronization}

% Konrad: Rewrote this paragraph. I think it's clearer now.
Extracting high-fidelity data from a wireless sensor network is
challenging for two primary reasons.  First, the radio links are lossy
and frequently asymmetrical.  Second, the low-cost crystal oscillators
on these nodes have low tolerances and therefore the clock rates vary
across the network.  Much prior research has focused on addressing
these challenges.

%% Extracting high-fidelity data from a sensor network means coping with lossy
%% radio links and accurately time-stamping sampled data.  Radio links in
%% wireless sensor networks are lossy and frequently asymmetrical, and low-cost
%% crystal oscillators deployed on wireless sensor network nodes have low
%% tolerances and clock rates vary across the network.  Much prior research has
%% focused on addressing these challenges.

% MDW: I found the fetch description to be somewhat confusing; tried
% to reorganize.

We developed a reliable data-collection protocol, called {\tt Fetch}, that
retrieves buffered data from each node over a multihop network. Samples are
buffered locally in {\em blocks} of 256~bytes, tagged with a sequence number
and timestamp. During transmission each requested block is fragmented into a
number of {\em chunks}, each sent in a single radio message. The base station
laptop retrieves a block by flooding a request to the network using {\tt
Drip}, a variant of the TinyOS {\tt Trickle}~\cite{trickle}
data-dissemination protocol.  The request contains the target node ID, the
block sequence number, and a bitmap identifying missing chunks in the block. 

The target node replies by sending the requested chunks over a multihop path
to the base station.  The routing tree is constructed using {\tt
MultiHopLQI}, a variant of the TinyOS {\tt MintRoute}~\cite{awoo-multihop}
routing protocol modified to select routes based on the CC2420 Link Quality
Indicator (LQI) metric.  Link-layer acknowledgments and retransmissions are
used at each hop to improve reliability.  Retrieving one minute of stored
data from a two-channel sensor node requires fetching 206~blocks and can
takes several minutes to complete, depending on the quality of the multihop
path and the node's depth in the routing tree.

% MDW: I think we paint too rosy of a picture here: we need to explain that
% FTSP does not always work lest the reader think we relied on it blindly,
% and point to future work.
Scientific volcano studies require that sampled data be accurately
timestamped; in our case, a global clock accuracy of 1~msec was sufficient.
We chose to use the Flooding Time Synchronization Protocol (FTSP)~\cite{ftsp}
from Vanderbilt University to establish a global clock across our network.
The published accuracy of FTSP is very high and the TinyOS code was
straightforward to integrate into our application. One of the nodes used a
Garmin GPS receiver to map the FTSP global time to GMT.  Unfortunately, FTSP
occasionally exhibited unexpected behavior which prevented nodes from
accurately timestamping some data. We are currently developing techniques to
correct the timestamps of our data set based on the large amount of status
messages logged from each node, which provide a mapping from the local clock
to the FTSP global time.
% MDW: This is a nice setup for your next paper...

\subsection{Command and Control}

% MDW: This is not universally true!
A feature missing from most traditional volcanic data acquisition equipment
is real-time network control and monitoring. The long-distance radio link
between the observatory and the sensor network allowed the laptop to monitor
and control the network's activity. We developed a Java-based GUI for
monitoring the network's behavior and manually setting parameters, such as
sampling rates and event detection thresholds.  In addition, the GUI was
responsible for controlling data collection following a triggered event,
moving significant complexity out of the sensor network.  All packets
received by the laptop from the sensor network were logged, facilitating
later analysis of the network's operation.

The GUI also displayed a table summarizing network state, based on the
periodic status messages transmitted by each node.  Each table entry included
the node ID; local and global timestamps; various status flags; the amount of
locally-stored data; depth, parent, and radio link quality in the routing
tree; and the node's temperature and battery voltage.  This functionality
greatly aided sensor deployment, as a team member could rapidly determine
whether a new node had joined the network and the quality of its radio
connectivity.

